<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Patrick Ding</title>
    <description>Patrick Ding&#39;s website. A blog with my projects and information about me.
</description>
    <link>http://delimited0.github.io/</link>
    <atom:link href="http://delimited0.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 05 Jul 2016 23:17:14 -0400</pubDate>
    <lastBuildDate>Tue, 05 Jul 2016 23:17:14 -0400</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Topic Modeling the Daily Princetonian</title>
        <description>&lt;p&gt;One of my favorite ways to waste time is reading internet comments. You can observe all kinds of angry people there and wonder at their pathologies. While I was still in school I particularly enjoyed reading the comments section of the Daily Princetonian because of its relevance to me. Though I only worked for a brief time formatting the print edition of the paper, I got to know some of the journalists, opinion writers, and editors through that work and through classes. Writing for a college newspaper requires a thick skin; the commenters are almost always out to get you.&lt;/p&gt;

&lt;p&gt;At some point I decided I would try to learn something about internet news and comments, perhaps so I could convince myself all those hours were not wasted. A basic question we can ask about news and comments is what are the paper and readers talking about? This kind of question, of what topics underly a corpus, is what probabilistic topic modeling can help answer. A frequently used method for topic modeling is latent Dirichlet allocation &lt;a href=&quot;#Blei:2003&quot;&gt;(Blei, Ng, &amp;amp; Jordan, 2003)&lt;/a&gt;. There have been many extensions to this original model, including some that focus on internet posts and their comments. In this post I describe a model introduced in &lt;a href=&quot;#Yano:2009&quot;&gt;(Yano, Cohen, &amp;amp; Smith, 2009)&lt;/a&gt; and apply it to a corpus of Daily Princetonian articles and their comments.&lt;/p&gt;

&lt;h1 id=&quot;data&quot;&gt;Data&lt;/h1&gt;

&lt;p&gt;I focus in particular on the opinion pieces and letters of the Daily Princetonian from fall 2013 to spring 2015–opinion pieces because in my experience these columns tend to attract the most, and the most rabid, comments, and that timespan because the Prince migrated its content to Wordpress in spring 2013 and all comments from before then have been lost. Using &lt;code class=&quot;highlighter-rouge&quot;&gt;beautifulsoup&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Selenium&lt;/code&gt; I scraped 962 articles from the opinion section with a total of 3576 comments. Using the &lt;code class=&quot;highlighter-rouge&quot;&gt;tm&lt;/code&gt; package in &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; I removed capitalization, whitespace, numbers, and punctuation, stemmed the words using the Porter stemmer &lt;a href=&quot;#Porter:1980&quot;&gt;(Porter, 1980)&lt;/a&gt;, and converted the documents to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bag-of-words_model&quot;&gt;bag of words&lt;/a&gt; representation required by Latent Dirichlet allocation.&lt;/p&gt;

&lt;h1 id=&quot;model&quot;&gt;Model&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;#Yano:2009&quot;&gt;(Yano, Cohen, &amp;amp; Smith, 2009)&lt;/a&gt; describe this model as LinkLDA. I do not use the other model they propose, CommentLDA, because it depends on commenter identity. The Disqus commenting system that the Prince uses allows guest commenters who can use anything as their usernames; thus it’s not possible to identify guests, who posted about 40% of the comments I consider here.&lt;/p&gt;

&lt;p&gt;The hierarchical representation of the model is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\theta_d &amp;\vert \alpha \sim Dir(\alpha) \\
	z_{dn} &amp;\vert \theta_d \sim Mult(\theta_d) \\
	\beta_k &amp;\vert \eta \sim Dir(\eta) \\
	w_{dn} &amp;\vert z_{dn}, \beta \sim Mult(\beta_{z_{dn}}) \\\
	z&#39;_{dcn} &amp;\vert \theta_d \sim Mult(\theta_d) \\
	\beta&#39;_k &amp;\vert \eta&#39; \sim Dir(\eta&#39;) \\
	w&#39;_{dcn} &amp;\vert z&#39;_{dcn}, \beta&#39; \sim Mult(\beta&#39;_{z&#39;_{dcn}})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;And in plate notation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2016-07-04-Daily-Prince-Actm/actm-gm.jpg&quot; alt=&quot;Model in plate notation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I implemented the model with &lt;code class=&quot;highlighter-rouge&quot;&gt;Rcpp&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;article-and-comment-topics&quot;&gt;Article and Comment Topics&lt;/h1&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;Tom&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &#39;Hi, Tom&#39; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Porter:1980&quot;&gt;Porter, M. F. (1980). An algorithm for suffix stripping. &lt;i&gt;Program&lt;/i&gt;, &lt;i&gt;14&lt;/i&gt;(3), 130–137. Retrieved from http://dblp.uni-trier.de/db/journals/program/program14.html#Porter80&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Blei:2003&quot;&gt;Blei, D. M., Ng, A. Y., &amp;amp; Jordan, M. I. (2003). Latent Dirichlet Allocation. &lt;i&gt;J. Mach. Learn. Res.&lt;/i&gt;, &lt;i&gt;3&lt;/i&gt;, 993–1022. Retrieved from http://dl.acm.org/citation.cfm?id=944919.944937&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yano:2009&quot;&gt;Yano, T., Cohen, W. W., &amp;amp; Smith, N. A. (2009). Predicting Response to Political Blog Posts with Topic Models. In &lt;i&gt;Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics&lt;/i&gt; (pp. 477–485). Stroudsburg, PA, USA: Association for Computational Linguistics. Retrieved from http://dl.acm.org/citation.cfm?id=1620754.1620824&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;

</description>
        <pubDate>Mon, 04 Jul 2016 00:00:00 -0400</pubDate>
        <link>http://delimited0.github.io/prince,/topic-models/2016/07/04/Daily-Prince-Actm.html</link>
        <guid isPermaLink="true">http://delimited0.github.io/prince,/topic-models/2016/07/04/Daily-Prince-Actm.html</guid>
        
        
        <category>prince,</category>
        
        <category>topic-models</category>
        
      </item>
    
  </channel>
</rss>
